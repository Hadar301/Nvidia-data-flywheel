# Custom values for deploying Data Flywheel to OpenShift cluster
# Uses data-flywheel bundled infrastructure (Elasticsearch, Redis, MongoDB)
# This file enables maximum use of the data-flywheel Helm chart

namespace: "hacohen-flywheel"

# Deployment profile settings
profile:
  production:
    enabled: false    # Enable Flower for debugging
  mlflow:
    COMPOSE_PROFILES: "mlflow"

# Image pull secrets - using your NGC API key
imagePullSecrets:
  - name: nvcrimagepullsecret
    registry: nvcr.io
    username: $oauthtoken
    password: ""  # Set via --set during install

# Secret configurations
secrets:
  ngcApiKey: ""       # Set via --set secrets.ngcApiKey=$NGC_API_KEY
  nvidiaApiKey: ""    # Set via --set secrets.nvidiaApiKey=$NVIDIA_API_KEY
  hfToken: ""         # Set via --set secrets.hfToken=$HF_TOKEN
  llmJudgeApiKey: ""  # Set via --set secrets.llmJudgeApiKey=$LLM_JUDGE_API_KEY
  embApiKey: ""       # Set via --set secrets.embApiKey=$EMB_API_KEY

# Enable Elasticsearch - using bundled deployment from data-flywheel chart
elasticsearch:
  enabled: true
  image:
    repository: docker.elastic.co/elasticsearch/elasticsearch
    tag: 8.12.2
  fullnameOverride: df-elasticsearch
  service:
    type: ClusterIP
    port: 9200
  resources:
    requests:
      memory: 2Gi
      cpu: 1
      ephemeral-storage: "5Gi"
    limits:
      memory: 4Gi
      cpu: 2
      ephemeral-storage: "10Gi"
  env:
    "ES_JAVA_OPTS": "-Xms2g -Xmx2g"
    "network.host": "0.0.0.0"
    "xpack.security.enabled": "false"
    "discovery.type": "single-node"
    "cluster.routing.allocation.disk.watermark.low": "99%"
    "cluster.routing.allocation.disk.watermark.high": "99%"
    "cluster.routing.allocation.disk.watermark.flood_stage": "99%"
    "logger.org.elasticsearch": "ERROR"
    "logger.org.elasticsearch.cluster": "ERROR"
    "logger.org.elasticsearch.discovery": "ERROR"
    "logger.org.elasticsearch.gateway": "ERROR"
    "logger.org.elasticsearch.indices": "ERROR"
    "logger.org.elasticsearch.node": "ERROR"
    "logger.org.elasticsearch.transport": "ERROR"
    "action.destructive_requires_name": "false"

# Enable MongoDB - using bundled deployment from data-flywheel chart
mongodb:
  enabled: true
  fullnameOverride: "df-mongodb"
  image:
    repository: docker.io/library/mongo
    tag: "7.0"
  service:
    type: ClusterIP
    port: 27017
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
      ephemeral-storage: "2Gi"
    limits:
      memory: "2Gi"
      cpu: "1"
      ephemeral-storage: "5Gi"

# Enable Redis - using bundled deployment from data-flywheel chart
redis:
  enabled: true
  image:
    repository: docker.io/library/redis
    tag: "7.2-alpine"
  fullnameOverride: "df-redis"
  service:
    type: ClusterIP
    port: 6379
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
      ephemeral-storage: "2Gi"
    limits:
      memory: "2Gi"
      cpu: "1"
      ephemeral-storage: "5Gi"

# Kibana - disabled (not required for Data Flywheel)
kibana:
  enabled: false

# Data Flywheel server configuration
foundationalFlywheelServer:
  image:
    repository: nvcr.io/nvidia/blueprint/foundational-flywheel-server
    tag: "0.3.0"
  imagePullSecrets:
    - name: nvcrimagepullsecret

  deployments:
    api:
      enabled: true
      fullnameOverride: "df-api"
      service:
        type: ClusterIP
        port: 8000
      env:
        # Use bundled Elasticsearch service (from data-flywheel chart)
        ELASTICSEARCH_URL: "http://df-elasticsearch-service:9200"
        # Use bundled Redis service (from data-flywheel chart)
        REDIS_URL: "redis://df-redis-service:6379/0"
        # Use bundled MongoDB service (from data-flywheel chart)
        MONGODB_URL: "mongodb://df-mongodb-service:27017"
        MONGODB_DB: "flywheel"
        ES_COLLECTION_NAME: "flywheel"
        # Set HOME to writable directory for OpenShift
        HOME: "/tmp"
      resources:
        requests:
          memory: "1Gi"
          cpu: "1"
          ephemeral-storage: "10Gi"
        limits:
          memory: "2Gi"
          cpu: "2"
          ephemeral-storage: "20Gi"
      command:
        - "uv"
        - "run"
        - "uvicorn"
        - "src.app:app"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8000"

    celeryWorker:
      enabled: true
      fullnameOverride: "df-celery-worker"
      env:
        ELASTICSEARCH_URL: "http://df-elasticsearch-service:9200"
        REDIS_URL: "redis://df-redis-service:6379/0"
        MONGODB_URL: "mongodb://df-mongodb-service:27017"
        MONGODB_DB: "flywheel"
        ES_COLLECTION_NAME: "flywheel"
        HOME: "/tmp"
      resources:
        requests:
          memory: "10Gi"
          cpu: "4"
          ephemeral-storage: "10Gi"
        limits:
          memory: "20Gi"
          cpu: "8"
          ephemeral-storage: "20Gi"
      command:
        - "uv"
        - "run"
        - "celery"
        - "-A"
        - "src.tasks.cli:celery_app"
        - "worker"
        - "--loglevel=info"
        - "--concurrency=50"
        - "--queues=celery"
        - "-n"
        - "main_worker@%h"
        - "--purge"

    celeryParentWorker:
      enabled: true
      fullnameOverride: "df-celery-parent-worker"
      env:
        ELASTICSEARCH_URL: "http://df-elasticsearch-service:9200"
        REDIS_URL: "redis://df-redis-service:6379/0"
        MONGODB_URL: "mongodb://df-mongodb-service:27017"
        MONGODB_DB: "flywheel"
        ES_COLLECTION_NAME: "flywheel"
        HOME: "/tmp"
      resources:
        requests:
          memory: "1Gi"
          cpu: "1"
          ephemeral-storage: "10Gi"
        limits:
          memory: "2Gi"
          cpu: "2"
          ephemeral-storage: "20Gi"
      command:
        - "uv"
        - "run"
        - "celery"
        - "-A"
        - "src.tasks.cli:celery_app"
        - "worker"
        - "--loglevel=info"
        - "--concurrency=1"
        - "--queues=parent_queue"
        - "-n"
        - "parent_worker@%h"
        - "--purge"

    mlflow:
      fullnameOverride: "df-mlflow"
      image: ghcr.io/mlflow/mlflow:v2.22.0
      service:
        type: ClusterIP
        port: 5000
      resources:
        requests:
          memory: "1Gi"
          cpu: "500m"
          ephemeral-storage: "2Gi"
        limits:
          memory: "2Gi"
          cpu: "2"
          ephemeral-storage: "5Gi"
      command:
        - "mlflow"
        - "server"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "5000"

    flower:
      fullnameOverride: "df-flower"
      service:
        type: ClusterIP
        port: 5555
      env:
        ELASTICSEARCH_URL: "http://df-elasticsearch-service:9200"
        REDIS_URL: "redis://df-redis-service:6379/0"
        MONGODB_URL: "mongodb://df-mongodb-service:27017"
        MONGODB_DB: "flywheel"
        ES_COLLECTION_NAME: "flywheel"
        HOME: "/tmp"
      resources:
        requests:
          memory: "1Gi"
          cpu: "1"
          ephemeral-storage: "2Gi"
        limits:
          memory: "2Gi"
          cpu: "2"
          ephemeral-storage: "5Gi"
      command:
        - "uv"
        - "run"
        - "celery"
        - "-A"
        - "src.tasks.tasks"
        - "flower"
        - "--port=5555"

  # Configuration for Data Flywheel - using nemo-gateway (deployed separately)
  config:
    nmp_config:
      # Use nemo-gateway service (deployed separately via deploy/nemo-gateway/)
      nemo_base_url: "http://nemo-gateway"
      # Use existing NIM service
      nim_base_url: "http://meta-llama3-1b-instruct:8000"
      # Datastore also goes through gateway
      datastore_base_url: "http://nemo-gateway"
      # Your namespace
      nmp_namespace: "hacohen-flywheel"

    logging_config:
      level: "INFO"

    mlflow_config:
      tracking_uri: "http://df-mlflow-service:5000"
      experiment_name_prefix: "data-flywheel"
      artifact_location: "./mlruns"

    # Use remote LLM judge to save cluster resources
    llm_judge_config:
      deployment_type: "remote"
      url: "https://integrate.api.nvidia.com/v1/chat/completions"
      model_name: "meta/llama-3.3-70b-instruct"

    # Reference existing NIM - mark as already deployed
    nims:
      - model_name: "meta/llama-3.2-1b-instruct"
        model_type: "llm"
        context_length: 8192
        gpus: 1
        pvc_size: 25Gi
        tag: "1.8.3"
        customization_enabled: true
        customizer_configs:
          target: "meta/llama-3.2-1b-instruct@2.0"
          gpus: 1
          max_seq_length: 8192

    data_split_config:
      eval_size: 100
      val_ratio: 0.1
      min_total_records: 50
      random_seed: null
      limit: 1000
      parse_function_arguments: true

    icl_config:
      max_context_length: 32768
      reserved_tokens: 4096
      max_examples: 3
      min_examples: 1
      example_selection: "semantic_similarity"
      similarity_config:
        relevance_ratio: 0.7
        embedding_nim_config:
          # Use remote embedding service
          deployment_type: "remote"
          url: "https://integrate.api.nvidia.com/v1/embeddings"
          model_name: "nvidia/llama-3.2-nv-embedqa-1b-v2"

    training_config:
      training_type: "sft"
      finetuning_type: "lora"
      epochs: 2
      batch_size: 16
      learning_rate: 0.0001

    lora_config:
      adapter_dim: 32
      adapter_dropout: 0.1

# Disable NeMo Microservices sub-chart - we're using existing NeMo infrastructure
nemo-microservices-helm-chart:
  enabled: false
